<p align="center">
  <img src="https://readme-typing-svg.herokuapp.com?font=Source+Code+Pro&size=32&duration=3000&pause=1000&color=2F80ED&center=true&vCenter=true&multiline=true&width=800&height=100&lines=👋+Hi+I'm+Shubham+Patil;Big+Data+%7C+Cloud+%7C+ML+Explorer+from+India" alt="Typing SVG" />
</p>

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=rect&color=0f2027,203a43,2c5364&height=100&section=header&text=Shubham%20Patil%20🚀&fontSize=45&fontAlign=50&fontColor=ffffff" />
</p>


# 👨‍💻 Shubham Patil – Big Data & Cloud Enthusiast

![Visitors](https://komarev.com/ghpvc/?username=shubhampatilmca&style=flat-square&color=blue)
![Status](https://img.shields.io/badge/Big%20Data-PySpark|Hadoop|Hive-blueviolet)
![Badge](https://img.shields.io/badge/Cloud-AWS|Azure|GCP-orange)
![Learning](https://img.shields.io/badge/Learning-Kafka|ML|R-informational)

---

Hi there! 👋  
I'm **Shubham Patil** from Pune, India — a passionate Big Data enthusiast, cloud learner, and future-ready data professional. I'm currently pursuing **PG-DAC in Big Data Analytics at CDAC Kharghar** while expanding my skills via **TrendyTech’s Big Data Master’s Program**.

---

## 🎓 Academic & Professional Journey

- 🎓 **MCA** – Sinhgad Institute, Pune (CGPA: 8.04)  
- 🎓 **BBA(CA)** – SM Joshi College, Pune (CGPA: 8.88)  
- 🚀 **CDAC – PG-DAC (Mar 2025)** – Specialization: *Big Data Analytics*

---

## 💼 My Tech Toolbox

### 🛠️ Languages & Tools
`Python` | `Java` | `SQL` | `Linux` | `Shell Scripting`

### 💾 Big Data Stack
`Hadoop` • `HDFS` • `MapReduce` • `Hive` • `Spark` • `PySpark` • `SparkSQL`

### ☁️ Cloud & DevOps
`AWS EC2` • `S3` • `IAM` • `VPC` • `Azure` • `GCP` • `Git` • `Agile`

---

## 📌 My CDAC Journey Highlights

- ✅ Learned core modules:
  - Java, DBMS, DS, OS, SE, AWS, Web Tech
- 🚀 Specialized in:
  - PySpark, Hive, Hadoop, MapReduce
- 📊 Built real-time project pipelines on production datasets

---

## 🔥 Noteworthy Projects

### 🧾 Lending Club Loan Analysis – *PySpark Project*
> Built a complete ETL pipeline for 2M+ loan records using PySpark.  
> Cleaned, transformed, and analyzed data to detect loan risks and defaults.  
> Used schema inference, null handling, deduplication, and hashing for scalable data processing.

---

## 📈 Currently Learning

- 📦 **Apache Kafka & Spark Streaming**  
- 📊 **Data Visualization in R**  
- 🤖 **ML on Big Data**  
- ☁️ **Advanced AWS – Athena, Glue, CloudFormation**

---

## 🌟 Soft Skills

- ✅ Growth mindset and fast learner  
- ✅ Strong team collaboration & problem-solving  
- ✅ Excellent time management and communication

---

## 📫 Connect with Me

- 📧 **Email**: shubham22016@gmail.com  
- 🔗 **LinkedIn**: [linkedin.com/in/shubhampatilmca](https://linkedin.com/in/shubhampatilmca)  
- 🌐 **Portfolio**: Coming Soon...  
- 💬 *Let’s collaborate on exciting Big Data & Cloud projects!*

---

<div align="center">

### ✨ *"Data is the new oil, but it’s crude. You need to refine it with skills, tools, and a curious mind!"* ✨

⭐ *Thanks for visiting!* ⭐  
📌 *Check out my repositories below ⬇️*

</div>
